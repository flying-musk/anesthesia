#!/usr/bin/env python
"""
æ¸¬è©¦Ollamaé€£æ¥è…³æœ¬
"""

import requests
import json

def test_ollama_connection():
    """æ¸¬è©¦Ollamaé€£æ¥"""
    print("ğŸ” æ¸¬è©¦Ollamaé€£æ¥...")
    
    try:
        # æ¸¬è©¦é€£æ¥
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        
        if response.status_code == 200:
            models = response.json()
            print("âœ… Ollamaæœå‹™æ­£åœ¨é‹è¡Œ")
            print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {len(models.get('models', []))}å€‹")
            
            for model in models.get('models', []):
                print(f"  - {model['name']}")
            
            return True
        else:
            print(f"âŒ Ollamaæœå‹™å›æ‡‰éŒ¯èª¤: {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("âŒ ç„¡æ³•é€£æ¥åˆ°Ollamaæœå‹™")
        print("ğŸ’¡ è«‹ç¢ºä¿Ollamaæ­£åœ¨é‹è¡Œ: ollama serve")
        return False
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {str(e)}")
        return False

def test_model_generation():
    """æ¸¬è©¦æ¨¡å‹ç”Ÿæˆ"""
    print("\nğŸ¤– æ¸¬è©¦æ¨¡å‹ç”Ÿæˆ...")
    
    try:
        payload = {
            "model": "qwen2.5:7b",
            "prompt": "è«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡çŸ­ä»‹ç´¹ä»€éº¼æ˜¯éº»é†‰ã€‚",
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9
            }
        }
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result.get("response", "")
            print("âœ… æ¨¡å‹ç”ŸæˆæˆåŠŸ")
            print(f"ğŸ“ å›æ‡‰å…§å®¹: {content[:200]}...")
            return True
        else:
            print(f"âŒ æ¨¡å‹ç”Ÿæˆå¤±æ•—: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ Ollamaæ¸¬è©¦é–‹å§‹...")
    print("=" * 50)
    
    # æ¸¬è©¦é€£æ¥
    connection_ok = test_ollama_connection()
    
    if connection_ok:
        # æ¸¬è©¦ç”Ÿæˆ
        generation_ok = test_model_generation()
        
        if generation_ok:
            print("\nğŸ‰ Ollamaæ¸¬è©¦å®Œå…¨æˆåŠŸï¼")
            print("âœ… ä½ ç¾åœ¨å¯ä»¥ä½¿ç”¨æœ¬åœ°LLMä¾†ç”Ÿæˆéº»é†‰é ˆçŸ¥äº†ï¼")
        else:
            print("\nâš ï¸ Ollamaé€£æ¥æ­£å¸¸ï¼Œä½†æ¨¡å‹ç”Ÿæˆæœ‰å•é¡Œ")
            print("ğŸ’¡ è«‹æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰: ollama pull qwen2.5:7b")
    else:
        print("\nâŒ Ollamaæ¸¬è©¦å¤±æ•—")
        print("ğŸ’¡ è«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿè¨­å®šOllama:")
        print("1. ä¸‹è¼‰ä¸¦å®‰è£Ollama: https://ollama.ai/download")
        print("2. å•Ÿå‹•æœå‹™: ollama serve")
        print("3. ä¸‹è¼‰æ¨¡å‹: ollama pull qwen2.5:7b")

if __name__ == "__main__":
    main()
